{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn7hqiPFbJrD5Qf5rxcMAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekta-kature/FMMLprojectsandlab/blob/main/MODULE_7_LAB_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iafzer7pAsvP"
      },
      "source": [
        "## Matrix Factorization (Based on Recommender System Example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HGtFwXPAyvW"
      },
      "source": [
        "Matrix factorization is a way to generate latent features when multiplying two different kinds of entities. Collaborative filtering is the application of matrix factorization useful to identify the relationship between items’ and users’ entities in a recommender system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wau9bcSBSGA"
      },
      "source": [
        "Define a set of Users (U), items (D), R size of |U|, and |D|. The matrix |U|\\*|D| includes all the ratings given by users. The goal is to discover K latent features. Given with the input of two matrics matrices P (|U|\\*k) and Q (|D|\\*k), it would generate the product result R."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvsHAiq8BnYk"
      },
      "source": [
        "Matrix P represents the association between a user and the features while matrix Q represents the association between an item and the features. We can get the prediction of a rating of an item by the calculation of the dot product of the two vectors corresponding to u_i and d_j."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcI45w46BuXB"
      },
      "source": [
        "To get two entities of both P and Q, we need to initialize the two matrices and calculate the difference of the product named as matrix M. Next, we minimize the difference through the iterations. The method is called gradient descent, aiming at finding a local minimum of the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEh6o6AV-HID"
      },
      "source": [
        "import numpy\n",
        "\n",
        "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
        "    '''\n",
        "    R: rating matrix\n",
        "    P: |U| * K (User features matrix)\n",
        "    Q: |D| * K (Item features matrix)\n",
        "    K: latent features\n",
        "    steps: iterations\n",
        "    alpha: learning rate\n",
        "    beta: regularization parameter'''\n",
        "    Q = Q.T\n",
        "\n",
        "    for step in range(steps):\n",
        "        for i in range(len(R)):\n",
        "            for j in range(len(R[i])):\n",
        "                if R[i][j] > 0:\n",
        "                    # calculate error\n",
        "                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])\n",
        "\n",
        "                    for k in range(K):\n",
        "                        # calculate gradient with a and beta parameter\n",
        "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
        "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
        "\n",
        "        eR = numpy.dot(P,Q)\n",
        "        e = 0\n",
        "        for i in range(len(R)):\n",
        "            for j in range(len(R[i])):\n",
        "                if R[i][j] > 0:\n",
        "                    e = e + pow(R[i][j] - numpy.dot(P[i,:],Q[:,j]), 2)\n",
        "                    for k in range(K):\n",
        "                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))\n",
        "        # 0.001: local minimum\n",
        "        if e < 0.001:\n",
        "            break\n",
        "\n",
        "    return P, Q.T"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wze_fjKCRz5"
      },
      "source": [
        "R = [\n",
        "     [5,3,0,1],\n",
        "     [4,0,0,1],\n",
        "     [1,1,0,5],\n",
        "     [1,0,0,4],\n",
        "     [0,1,5,4],\n",
        "     [2,1,3,0],\n",
        "    ]\n",
        "\n",
        "R = numpy.array(R)\n",
        "# N: num of User\n",
        "N = len(R)\n",
        "# M: num of Movie\n",
        "M = len(R[0])\n",
        "# Num of Features\n",
        "K = 3\n",
        "\n",
        "\n",
        "P = numpy.random.rand(N,K)\n",
        "Q = numpy.random.rand(M,K)\n",
        "\n",
        "\n",
        "\n",
        "nP, nQ = matrix_factorization(R, P, Q, K)\n",
        "\n",
        "nR = numpy.dot(nP, nQ.T)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P51JmjVPCg_y"
      },
      "source": [
        "The predicted matrix is generated below. As you can see, the predicted matrix has similar output with the true values, and the 0 ratings are replaced with the prediction based on the similar users’ preferences on movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhPzLvuhCZzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f84335-7af4-4d16-8a8e-da3461f67397"
      },
      "source": [
        "print(nR)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.01155914 2.91672283 3.2724365  0.99997381]\n",
            " [3.97510522 2.32481727 2.72629974 0.99877629]\n",
            " [1.08204757 0.84216687 5.15979982 4.96429206]\n",
            " [0.98703173 0.74813171 4.04999068 3.97491315]\n",
            " [1.62303111 1.09602199 4.93972781 4.0215329 ]\n",
            " [1.86987574 1.1767911  3.00783174 2.3213843 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYPp7vZuDF3H"
      },
      "source": [
        "## SVD (Continuing the Recommender System Example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxPbbpPODQXY"
      },
      "source": [
        "When it comes to dimensionality reduction, the Singular Value Decomposition (SVD) is a popular method in linear algebra for matrix factorization in machine learning. Such a method shrinks the space dimension from N-dimension to K-dimension (where K < N) and reduces the number of features. SVD constructs a matrix with the row of users and columns of items and the elements are given by the users’ ratings. Singular value decomposition decomposes a matrix into three other matrices and extracts the factors from the factorization of a high-level (user-item-rating) matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdX2UCYCDw4u"
      },
      "source": [
        "The factorisation of this matrix is done by the singular value decomposition. It finds factors of matrices from the factorisation of a high-level (user-item-rating) matrix. The singular value decomposition is a method of decomposing a matrix into three other matrices as given below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_HxawLkEFzL"
      },
      "source": [
        "Where A is a m x n utility matrix, U is a m x r orthogonal left singular matrix, which represents the relationship between users and latent factors, S is a r x r diagonal matrix, which describes the strength of each latent factor and V is a r x n diagonal right singular matrix, which indicates the similarity between items and latent factors. The latent factors here are the characteristics of the items, for example, the genre of the music. The SVD decreases the dimension of the utility matrix A by extracting its latent factors. It maps each user and each item into a r-dimensional latent space. This mapping facilitates a clear representation of relationships between users and items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJlAQUIWSlU5"
      },
      "source": [
        "## Working with Text 1 : Bag of Words Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PRkDXxsSyTv"
      },
      "source": [
        "A problem with modeling text is that it is messy, and techniques like machine learning algorithms prefer well defined fixed-length inputs and outputs. Machine learning algorithms cannot work with raw text directly; the text must be converted into numbers. Specifically, vectors of numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXXl_VVxS8Ip"
      },
      "source": [
        "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
        "\n",
        "The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
        "\n",
        "    A vocabulary of known words.\n",
        "    A measure of the presence of known words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNr4iHKTmOv"
      },
      "source": [
        "Let's try BoW model on an example\n",
        "\n",
        "The sentences are -\n",
        "\n",
        "1.   grey is a dull color\n",
        "2.   orange is a fruit\n",
        "3.   orange is a colour\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep8b9b7wCtu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d17008-98f3-41c4-8d0f-638eb3dcdc4f"
      },
      "source": [
        "sentences = [[\"grey\", \"is\", \"a\", \"dull\", \"colour\"], [\"orange\", \"is\", \"a\", \"fruit\"], [\"orange\", \"is\", \"a\", \"colour\"]]\n",
        "# Add your sentences here and see the embeddings\n",
        "\n",
        "# The vocabulary is -\n",
        "# vocabulary = [\"grey\", \"is\", \"a\", \"dull\", \"colour\", \"orange\", \"fruit\"]\n",
        "\n",
        "vocabulary = []\n",
        "for sentence in sentences:\n",
        "  for word in sentence:\n",
        "    if word not in vocabulary:\n",
        "      vocabulary.append(word)\n",
        "\n",
        "print(vocabulary)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['grey', 'is', 'a', 'dull', 'colour', 'orange', 'fruit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mi5MGLXUUpo"
      },
      "source": [
        "Now we need to find which words occur in a given sentence and hence assign each sentence a corresponding vector based upon multiple metrics.\n",
        "\n",
        "This can be done in a few ways :\n",
        "1. Boolean count of whether the word occurs in a sentence\n",
        "2. Counts of each word that occurs in a sentence\n",
        "3. Frequency of words out of all oher words in a sentence\n",
        "4. TF-IDF (will be covered in next lab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvokt1ZmUJsa"
      },
      "source": [
        "# We shall implement point 2 now\n",
        "\n",
        "bow_sentences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  bow_representation = [0 for i in range(len(vocabulary))]\n",
        "  for indx, word in enumerate(vocabulary):\n",
        "    if word in sentence:\n",
        "      bow_representation[indx]+=1\n",
        "  bow_sentences.append(bow_representation)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUZEFrN8W_Rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66cf388-88f2-4dcd-eb34-623fd6948623"
      },
      "source": [
        "print(bow_sentences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 1, 1, 1, 0, 0], [0, 1, 1, 0, 0, 1, 1], [0, 1, 1, 0, 1, 1, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt7b5JhbXHjj"
      },
      "source": [
        "**You can now try to add your sentences and see how it works!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLEr5EK0XPH1"
      },
      "source": [
        "## Working with Text 2 : LSI (Latent Semantic Indexing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9b76qrJYHFH"
      },
      "source": [
        "Now, we will introduce an indexing and retrieval method: the Latent Semantic Indexing (LSI). It uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in a corpus (unstructured collection of documents).\n",
        "\n",
        "Latent Semantic Indexing is a common technique in the NLP field. It is used to analyze relationships between a set of documents and the terms they contain in order to produce a set of concepts related to the documents and terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKGOSpS1YPrz"
      },
      "source": [
        "LSI is based on the distributional hypothesis which states that words that are close in meaning will occur in similar pieces of text. The starting point is the representation matrix $A$ of the distribution of the words within the set of documents. It is a $m * n$ matrix where $m$ is the number of unique words and $n$ is the corpus cardinality. The element $a_{ij}$ represents the frequency of the word $i$ in the document $j$.\n",
        "\n",
        "Singular Value Decomposition (SVD) is consequently applied to the matrix A in order to reduce the dimensionality of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7_wuOReY0ah"
      },
      "source": [
        "The SVD computes the term and document vector spaces by approximating the single term-frequency matrix $A$ as follows:\n",
        "\n",
        "$$\n",
        "A \\approx TSD^T\n",
        "$$\n",
        "\n",
        "where $T$ is the $m * r$ term-concept vector matrix, $S$ is the $r * r$ singular values matrix, $D$ is the $n * r$ concept-document vector matrix, such that\n",
        "\n",
        "$$\n",
        "T^{T}T = I_r \\\\\n",
        "D^{T}D = I_r \\\\\n",
        "S_{11}\\ge S_{22}\\ge ... \\ge S_{rr} \\ge 0 \\\\ S_{ij}=0 \\hspace{1cm} \\forall i \\ne j\n",
        "$$\n",
        "\n",
        "The next step is to truncate the SVD and keep only the largest $k$ << $r$ diagonal entries in the singular value matrix $S$, where $k$ is typically on the order 100 to 300 dimensions. This effectively reduces the term matrix $T$ size to $m * k$ and the document matrix $D$ size to $n * k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNNb2E9IqAU8"
      },
      "source": [
        "Let us now get started with the code! Note that we will use a library **gensim**, which does most of the hard work for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaUepUbPXDXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde45028-25d8-4301-8a71-a29601b68cdd"
      },
      "source": [
        "# Install requires lirbaries\n",
        "\n",
        "!pip install --upgrade gensim\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import preprocess_documents"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KChn5d4Uk2Gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabcc0e3-9846-4c4d-caeb-54a890daa9c4"
      },
      "source": [
        "!curl -L -o 'movies.csv' 'https://drive.google.com/uc?export=downloads&id=12k4ltUwdhg525XW4dT6hgW18t_Fil3Hj'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 77.4M  100 77.4M    0     0  18.2M      0  0:00:04  0:00:04 --:--:-- 28.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NozEZe9Dbcqz"
      },
      "source": [
        "df = pd.read_csv('movies.csv', sep=',', usecols = ['Release Year', 'Title', 'Plot'])\n",
        "df = df[df['Release Year'] >= 2000] # Use a subset of the data\n",
        "text_corpus = df['Plot'].values # Get the movie plots"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pQSHl6SbxHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdfd129-feda-4a60-a661-b46a73f0d016"
      },
      "source": [
        "print(text_corpus)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"After three years in prison, Cruella de Vil has been cured of her desire for fur coats by Dr. Pavlov and is released into the custody of the probation office on the provision that she will be forced to pay the remainder of her fortune (eight million pounds) to all the dog shelters in the borough of Westminster should she repeat her crime. Cruella therefore mends her working relationship with her valet Alonzo and has him lock away all her fur coats. Cruella's probation officer, Chloe Simon, nevertheless suspects her, partly because Chloe is the owner of the now-adult Dipstick (one of the original 15 puppies from the previous film) who moved from Roger and Anita's house to her house.\\r\\nDipstick's mate, Dottie, has recently given birth to three puppies: Domino, Little Dipper and Oddball (who lacks spots). To mend her reputation, Cruella buys the Second Chance Dog shelter, owned by Kevin Shepherd, to resolve its financial insolvency that is on the verge of eviction. Meanwhile, Dr. Pavlov discovers that when his therapy's subjects are subjected to loud noises, they revert to their original states but conceals this discovery. When Big Ben rings in her presence, Cruella reverts to her former personality and enlists the help of French furrier Jean-Pierre LePelt to steal 102 Dalmatian puppies for a new fur coat with a hood.\\r\\nWhen Kevin tells Chloe that if Cruella violates her parole, her entire fortune will go to him, since his dog shelter is the only one in the borough of Westminster, Cruella has Kevin framed for the theft of the puppies and invites Chloe and Dipstick to dinner while LePelt steals Dottie and her three puppies. Dipstick hurries back to the apartment and hides in LePelt's truck but is later captured at the train station. Chloe rushes home to save her pets but arrives too late. She is joined by Kevin, who has escaped from prison with help from his dogs and talking parrot, Waddlesworth.\\r\\nUpon finding a ticket for the Orient Express to Paris dropped by LePelt, Kevin and Chloe attempt and fail to stop Cruella and LePelt, but Oddball and Waddlesworth pursue their enemies secretly. In Paris, Kevin and Chloe save some of the captive puppies, but they are seen and locked in the cellar just as the puppies flee. Cruella goes after the puppies alone. Alonzo, when scolded beyond his patience and had enough of being abused, defeats LePelt and frees Kevin and Chloe and they give chase to a bakery, where the puppies and Kevin's dogs imprison Cruella in an immense cake. She and LePelt are thereupon arrested.\\r\\nKevin and Chloe are personally awarded the remnants of Cruella's fortune by Alonzo himself and Oddball's coat finally develops spots.\"\n",
            " \"Gwen Cummings (Sandra Bullock) spends her nights in a drunken haze with her boyfriend, Jasper (Dominic West). She ruins her sister (Elizabeth Perkins) Lily's wedding by showing up late and disheveled, delivering a drunken, rambling speech, and knocking over the wedding cake. Intoxicated, Gwen steals a limousine from the reception, tries to locate a cake store, and winds up losing control of the car and crashing into a house. She is given a choice between 28 days in jail or in a rehab center, and she chooses rehab.\\r\\nGwen is introduced to a variety of patients while in treatment: Oliver (Mike O'Malley) (a hypersexual cocaine addict), Daniel (Reni Santoni) and Roshanda (Marianne Jean-Baptiste) (alcoholics), Bobbi Jean (Diane Ladd) (an older addict), Gerhardt (Alan Tudyk) (a gay man whose addiction is not specified), and Cornell (Steve Buscemi), the rehab facility's director (a recovered drug addict and alcoholic). Her roommate is young Andrea (Azura Skye), a heroin addict who sporadically self-harms and is a fan of the fictitious soap opera Santa Cruz.\\r\\nInitially, Gwen is angry and resistant to taking part in any of the treatment programs on offer, refusing to admit that she is an alcoholic. On visiting day, Jasper shows up and slips her a bottle of medication, then the two sneak off for a day of drinking and drugging. Later, Gwen returns to the facility, clearly inebriated. The next day, Gwen is confronted by Cornell. He informs her that she's being kicked out of rehab the next day and will be going to jail instead. Gwen angrily denies that she has a problem with alcohol, that she can stop anytime that she wants. Ignored, she angrily makes her way back to her room, where she rifles through her tissue box to get to her smuggled drugs. She puts a pill in her mouth but quickly spits it back out, then tosses the open bottle out the third-story window.\\r\\nAll throughout the day, Gwen experiences withdrawal symptoms. She shuns the meetings and any activities, all the while desperately trying to push through her physical discomfort on her own. Later that evening, in a moment of weakness, she attempts to climb out her window and retrieve the discarded meds. She falls, severely spraining her ankle, and is rescued by Eddie (Viggo Mortensen), a pro baseball player and fellow addict, who is just arriving as a new patient.\\r\\nThe next morning, Gwen asks Cornell for another chance, finally convinced that anyone who would climb out of a three-story window to chase a high might have a problem. He relents; and Gwen finally begins to participate in the recovery process, growing closer to her fellow addicts and her roommate, Andrea. Gwen discovers that Eddie is also a fan of Santa Cruz, and their fellow group participants join Eddie and Andrea in catching up on tapes of the show. During therapy sessions, Gwen experiences flashbacks of a childhood that included a thrill-seeking addict mother who died of an overdose when Gwen was about six, leaving young Lily and Gwen to be raised by an aunt.\\r\\nOn one of his visits, Jasper proposes to Gwen, bringing champagne to celebrate. Not wanting to jeopardize her newfound sobriety, Gwen throws the champagne into the lake. Later, her fellow addicts try to encourage her to see that Jasper isn't taking her sobriety seriously and to be careful. At some point, Gwen's sister Lily attends a group therapy session but leaves in disgust when Gwen become dismissive of Lily's recollections and resentments of her younger sister's drunken antics.\\r\\nEddie and Gwen's friendship grows closer. Afraid to share what she'd done as a practicing alcoholic for fear of looking bad to Eddie, they share a moment when Eddie tells her that's just what she's done. She is just fine as she is. They are come upon by Jasper, who showed up unannounced. Jasper then proceeds to insult and pick a fight with Eddie, shoving him. Eddie punches him before Gwen stops any further violence. Eddie walks off, and Gwen and Eddie's friendship becomes estranged.\\r\\nGwen's roommate, Andrea, is soon to be released and has been agitated and moody at the prospect, as well as heartbroken that her mother has never visited her during her entire stint in rehab. Gwen discovers Andrea dead in their bathroom, clearly having overdosed. Andrea's death leaves Gwen devastated and perhaps wiser as to how an addict's behavior affects others. Gwen commits herself to restoring her relationship with her sister. Gwen and Lily reconcile, and Gwen leaves treatment, but not before Eddie warns her that Jasper is dangerous to her sobriety.\\r\\nBack in New York, Jasper tries to make amends to Gwen for his behavior. Reconciling, Gwen tries to help Jasper to understand what needs to change in their relationship to support her recovery; but she soon sees that Jasper doesn't take her sobriety seriously. Seeing old party friends, Jasper wants to join them, demonstrating that he won't change his lifestyle or adjust to her needs and abstentions as a recovering addict. Gwen comes to terms with the fact that they are too different now and starts to see that recovery, though an everyday struggle, might be attainable. She breaks up with Jasper and walks away for good. Some time later, she is reunited with a sober Gerhardt at a floral shop. In a post-credit scene, Eddie recognizes a Santa Cruz character, Falcon, arrive as a new patient at the rehab facility.\"\n",
            " 'Robert Douglas (Brian Hooks) is in prison for one of two felonies he has previously committed. While in prison, he watches to a local news report that states California has instituted the habitual offenders law, commonly referred to as the \"three-strikes\" law, which will put offenders with three felonies in prison for a minimum of twenty-five years. On his last day in prison, Robert is ecstatic about being released, informing his girlfriend, Juanita (N\\'Bushe Wright), of such and contacting his friend, Tone (Faizon Love) to pick him up after he gets out.\\r\\nOn the way to pick up Robert, Tone stops to pick up a woman on the street and takes her home. Tone tells his friend, J.J. (De\\'Aundre Bonds), to pick Robert up in his place. Shortly after leaving prison, while on the way to check in with Robert\\'s probation officer, the pair are pulled over by police; J.J. reveals that the car they are driving is stolen and, unwilling to surrender, he begins shooting at the cops. Knowing he will be convicted for his third and final felony offense, Robert flees on foot. As he is trying to escape himself, Jay-Jay is wounded from a shot to the buttocks and is taken into custody. Robert escapes pursuit by hiding at a backyard party and, soon after returning home, learns he has been identified and implicated in the shooting as a suspect. Detective Jenkins (David Alan Grier) leads the investigation.\\r\\nWhile in the hospital, J.J. calls his friend, Blue (Barima McKnight), and berates Robert for leaving him during the shooting, telling Blue that he plans to peg Robert as the shooter when the police come to interview him. The call is recorded on voice mail at Blue\\'s home. Jay-jay antagonizes the man guarding his room, who then lets a homosexual janitor that was ogling J.J. from afar into the room. Unable to defend himself, it is assumed J.J. is sexually assaulted by the janitor.\\r\\nAt home, Robert receives a call from Tone. Parked outside Robert\\'s house, Tone blames him for leaving J.J. by himself during the shootout, and tells him he plans to pass the word around the neighborhood for everyone to be on the lookout for him, insinuating there would be repercussions. Robert reaches out to his probation officer for help in proving his innocence, but is told that his best option is to simply turn himself in.\\r\\nRobert gets into a heated argument with his father (George Wallace) and is kicked out of the house, but runs into his friend, Mike (E-40), who lends Robert enough money for him and Juanita to get a hotel room. Detective Jenkins and Officer Roberts (Dean Norris) stop by Robert\\'s parent\\'s house, but are turned away without a warrant to search the premises.\\r\\nThe following morning, Robert is informed by his mother that the police are searching for him, but more importantly that a woman named Dahlia (Mo\\'Nique) has information that is critical to proving his innocence and keeping himself out of jail: the tape recording of J.J.\\'s call to Blue. She tells Robert to meet him at her house. Robert meets up with Mike once more, and asks Mike to set him up with a good lawyer.\\r\\nAt her home, Dahlia agrees to hand over the tape, but reveals she\\'s had a crush on Robert since high school, and blackmails him into letting her have her way with him for the tape. Begrudgingly, Robert accepts her proposal. Having witnessed Robert enter Dahlia\\'s house, Blue - Dahlia\\'s brother - calls Tone, who brings several of his goons over to his location. As Robert sneaks out with the tape, Tone and his people are there to meet him, and begin to jump him. Just as quickly as it begins, the police show up and send a dog after Robert.\\r\\nRobert manages to get to his car, and a high-speed chase ensues. After being cornered in an alley by Detective Jenkins and several other pursuing units, Robert attempts to give himself up, but Jenkins begins shooting anyway; in the confusion, Robert manages to get away again, and is picked up by Mike. Chased by police cars and surveilled by helicopters, Robert\\'s chase is broadcast across every news network; he tells Mike to head to a church where he has told the lawyer Mike set him up with, Mr. Libowitz (Phil Morris), he will hand himself over to the authorities. The media and dozens of spectators are there as he arrives. Robert manages to be taken into custody without any harm done to him.\\r\\nAt his trial, the judge believes the tape recording clearly proves Robert was not the shooter and was completely unaware that the vehicle he and J.J. were in at the time was stolen. The felony charges against him are dismissed, and he avoids being convicted of a crime that would have put him behind bars under the \"three-strikes\" law. However, the opposition points out that Robert did not check in with his probation officer after leaving prison, and Robert is sentenced to 30 days in jail for violating his parole.\\r\\nBefore court is adjourned, Robert\\'s father tells him that he will personally pick him up after he is released. The epilogue states that Rob was eventually released from prison early due to overcrowding.'\n",
            " ...\n",
            " \"Zafer, a sailor living with his mother Döndü in a coastal village in Izmir, has just separated from his girlfriend Mehtap whose father is also a sailor. While Döndü and her friend, Fahriye try to help Zafer to marry someone and have his own family, a famous and talented actress, Aslı surprisingly attends Zafer's boat tour. Then Asli and Zafer find themselves getting to know each other.\"\n",
            " 'The film centres around a young woman named Amy Tyler, who books a surprise holiday to Europe with her boyfriend Josh Merit.[4] However, the two separate before they are able to go onto the trip; therefore, she advertises her tickets online in search for another man with an identical name to her ex-boyfriend to go on the tour instead.[3][1] The story, although written in 2012, bore similarities[1] to a 2014 case of a 28-year-old Toronto man named Jordan Axani, who offered up free tickets to on Reddit a girl named Elizabeth Gallagher, when he broke up with his girlfriend of the same name. Although his ticket had a strict no-transfer policy, as passport information was not required when booking, the ticket could be used by anyone with the same name.[5]\\r\\nWhen a BuzzFeed article about the story became viral,[6] the script was given \"new life\".[2] Brendan Bradley said that \"I wrote this script five years ago\", Bradley said, \"and everyone told me ... \\'This would never happen! This is too unrealistic!\\' And the project didn’t get any traction because everyone thought the premise was too crazy. And then it happened in real life.\"[1]'\n",
            " 'The writer Orhan Şahin returns to İstanbul after so many years to help the well-known director Deniz Soysal to write his first novel. Orhan finds himself to look with nostalgia at the places where he was born and raised, reliving the relationships with friends, family and past loves.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCbeuHM7bw4x"
      },
      "source": [
        "processed_corpus = preprocess_documents(text_corpus) # Preprocess the text\n",
        "dictionary = gensim.corpora.Dictionary(processed_corpus)\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus] # Find the BoW representations"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Lt8O52j0iM"
      },
      "source": [
        "tfidf = gensim.models.TfidfModel(bow_corpus, smartirs='npu') # Perform TF-IDF to get the matrix\n",
        "corpus_tfidf = tfidf[bow_corpus]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWsfVTL6j3Nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54654d05-5b68-445a-808f-179d7740f1ec"
      },
      "source": [
        "lsi = gensim.models.LsiModel(corpus_tfidf, num_topics=1000) # Perform LSI\n",
        "index = gensim.similarities.MatrixSimilarity(lsi[corpus_tfidf])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
            "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btVKvUuBj_GX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39eb6cc5-36ee-4e66-a4ee-a98dd07acb3c"
      },
      "source": [
        "new_doc = \"In the arcade at night the video game characters leave their games. The protagonist is a girl from a candy racing game who glitches\"\n",
        "# new_doc = \"Boy studies ballet in secret. His father wants him to go to the gym and boxe. They raise money for audition in London\"\n",
        "\n",
        "new_doc = gensim.parsing.preprocessing.preprocess_string(new_doc)\n",
        "new_vec = dictionary.doc2bow(new_doc)\n",
        "vec_bow_tfidf = tfidf[new_vec]\n",
        "vec_lsi = lsi[vec_bow_tfidf]\n",
        "sims = index[vec_lsi]\n",
        "for s in sorted(enumerate(sims), key=lambda item: -item[1])[:10]:\n",
        "    print(f\"{df['Title'].iloc[s[0]]} : {str(s[1])}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candlestick : 0.78073543\n",
            "Overlord: The Undead King : 0.7740027\n",
            "Overlord: The Dark Warrior : 0.7740027\n",
            "Inferno : 0.69323444\n",
            "Wreck-It Ralph : 0.69033784\n",
            "Kami-sama no Iu Toori : 0.67623675\n",
            "Sex Competition : 0.6597798\n",
            " Facing the Giants : 0.6025884\n",
            "Glory Road : 0.59998894\n",
            "Sleuth : 0.5733697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0X0MppJrKrR"
      },
      "source": [
        "As you can see we give the description of the movie \"Wreck-it Ralph\" and it appears as the third recommended. We are doing a great job with LSI!\n",
        "\n",
        "Feel free to change the description and see what movies you get :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxXwYyCkrls-"
      },
      "source": [
        "## Points to think about\n",
        "\n",
        "1. How exactly does matrix factorization help us in the recommendation procedure? Why can we not simply model the user-movie matrix?\n",
        "\n",
        "Ans. Matrix factorization helps us in the recommendation procedure by decomposing the original user-movie rating matrix into two lower-dimensional matrices: one representing user features and the other representing item (movie) features. This decomposition allows us to capture latent factors or features that influence user preferences and item characteristics. By learning these latent factors, we can effectively model the underlying structure of user-item interactions, even in the presence of sparse or incomplete data.\n",
        "\n",
        "Simply modeling the user-movie matrix directly may not be efficient because:\n",
        "\n",
        "The user-movie matrix is typically sparse, with many missing entries representing unrated movies by users.\n",
        "It doesn't capture underlying patterns or latent factors that influence user preferences and item characteristics.\n",
        "It may lead to overfitting or high dimensionality, especially when dealing with large datasets.\n",
        "Matrix factorization addresses these challenges by reducing dimensionality and learning latent factors, enabling more accurate and scalable recommendation systems.\n",
        "\n",
        "2. What do the rows of the matrix $T$ represent? (Definition of $T$ is above in the introduction to LSI).\n",
        "\n",
        "Ans. In Latent Semantic Indexing (LSI), the rows of the matrix\n",
        "T represent the term-concept vectors. Each row corresponds to a term (word) in the vocabulary, and the elements of the row represent the strength of association between the term and each latent concept (or topic) extracted from the corpus. In other words, T maps terms from the original vocabulary to a lower-dimensional space of latent concepts, where each concept represents a thematic dimension or underlying pattern in the text corpus. The values in the rows of T indicate the relevance or importance of each term to each latent concept.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}